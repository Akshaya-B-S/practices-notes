{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187543a8",
   "metadata": {},
   "source": [
    "CALCULUS FOR ML\n",
    "DERIVATES \n",
    "   - measure the rate at which the function changes with repect to its input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5edbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMON DERIVATES\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b93b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*x\n"
     ]
    }
   ],
   "source": [
    "x = sp.Symbol('x')\n",
    "f = x**2\n",
    "derivatives = sp.diff(f,x)\n",
    "print(derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e6785df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Derivatives: 2*x 2*y\n"
     ]
    }
   ],
   "source": [
    "#PARTIAL DERIVATES\n",
    "  # - measure how a function change with respect to one variable while keep other variable constant\n",
    "#GRADIENT\n",
    "  # - vector of all partial derivatives , indicating the directive of the steepest ascent\n",
    "x, y = sp.symbols('x y')\n",
    "f = x**2 + y**2\n",
    "grad_x = sp.diff(f,x)\n",
    "grad_y = sp.diff(f,y)\n",
    "print(\"Partial Derivatives:\",grad_x , grad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecbf47c",
   "metadata": {},
   "source": [
    "#GRADIENT DESCENT OPTIMIZATION ALGORITHM\n",
    " # -iterative optimization algorithm used to minimize the function\n",
    " # -update parameter in the direction of negative gradient to find the mininum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe56222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3*x**2 + 5\n"
     ]
    }
   ],
   "source": [
    "#compute derivates of basic function\n",
    "x = sp.Symbol('x')\n",
    "f = x**3 + 5*x +7\n",
    "deri = sp.diff(f,x)\n",
    "print(deri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0415ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad x: 2*x - 4*y\n",
      "Grad y: 3 - 4*x\n"
     ]
    }
   ],
   "source": [
    "#compute gradient\n",
    "#define a multi variable function\n",
    "x,y = sp.symbols('x y')\n",
    "f = x**2 + 3*y - 4*x*y\n",
    "grad_x = sp.diff(f,x)\n",
    "grad_y = sp.diff(f,y)\n",
    "print(\"Grad x:\", grad_x)\n",
    "print(\"Grad y:\", grad_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "392d4445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Parameter: [1.166663   0.75000161]\n"
     ]
    }
   ],
   "source": [
    "#implement gradient descent for linear regression\n",
    "import numpy as np\n",
    "def gradient_descent(x, y, theta, learning_rate, iterations):\n",
    "    m = len(y)\n",
    "    for _ in range(iterations):\n",
    "        predictions = np.dot(x,theta)\n",
    "        error = predictions -y\n",
    "        gradient = (1/m)*np.dot(x.T , error)\n",
    "        theta -= learning_rate *gradient\n",
    "    return theta\n",
    "x= np.array([[1,1],[1,2],[1,3]])\n",
    "y = np.array([2 ,2.5 ,3.5]) \n",
    "theta = np.array([0.1, 0.1])\n",
    "learning_rate =0.1\n",
    "iterations = 1000\n",
    "#perform gradient\n",
    "optimized_theta = gradient_descent(x, y, theta, learning_rate, iterations)\n",
    "print(\"Optimized Parameter:\", optimized_theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39d46a",
   "metadata": {},
   "source": [
    "INTEGRAL AND OPTIMIZATION\n",
    "INTEGRAL\n",
    "   - compute the area under the curve, representation accumulation\n",
    "   - used to compute probability distribution\n",
    "   - cost function\n",
    "OPTIMIZATION\n",
    "   *LOCAL MINIMA\n",
    "      - is a point where function has lower point than neighbor\n",
    "   *GLOBAL MINIMA\n",
    "      - absolute lower point acros the entire domain\n",
    "   *CONVEX FUNCTION\n",
    "      -  ensure that any local minima is also global minima\n",
    "   *NON CONVEX FUNCTION\n",
    "      - most neutral network loss function leading to challenge in optimization\n",
    "STOCHASTIC GRADIENT DESCENT(SGD)\n",
    "  - optimizatioin algorithm that use random subset of data to compute gradient and update parameter\n",
    "  VARIENT OF SGD\n",
    "  -MINI BATCH SGB - update parameter using small batches instead of small sample\n",
    "  -MOMENTUM - add fraction of previous update to current updte to accelerate convergence\n",
    "  -ADAM OPTIMIZER - used quite lot, combine momentum with adaptive learning rate for faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8c18e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definite_integral 8/3\n",
      "Indefinite_integral x**3/3\n"
     ]
    }
   ],
   "source": [
    "x =sp.Symbol('x')\n",
    "f = x**2\n",
    "definite_integral = sp.integrate(f,(x,0,2))\n",
    "indefinite_integral = sp.integrate(f,x)\n",
    "print(\"definite_integral\",definite_integral)\n",
    "print(\"Indefinite_integral\", indefinite_integral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c545d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indefinite integral -exp(-x)\n",
      "definite integral 1\n"
     ]
    }
   ],
   "source": [
    "#calculate integral of simple function\n",
    "x = sp.Symbol('x')\n",
    "f = sp.exp(-x)\n",
    "#compute indefinite integral\n",
    "indef_integral = sp.integrate(f,x)\n",
    "print(\"Indefinite integral\",indef_integral)\n",
    "def_integral = sp.integrate(f,(x, 0, sp.oo))\n",
    "print(\"definite integral\",def_integral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78725a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta optimization: [[4.52573676]\n",
      " [3.0045972 ]]\n"
     ]
    }
   ],
   "source": [
    "#implement SGD for linear regression\n",
    "np.random.seed(42)\n",
    "x = 2*np.random.rand(100,1)\n",
    "y = 4 + 3 * x + np.random.rand(100,1)\n",
    "x_b = np.c_[np.ones((100,1)),x]    #add bias to x\n",
    "def stochastic_gradient(x,y, theta, learning_rate, n_epocs):\n",
    "    m=len(y)\n",
    "    for epoch in range(n_epocs):\n",
    "         for i in range(m):\n",
    "              random_index = np.random.randint(m)\n",
    "              xi = x[random_index: random_index +1]\n",
    "              yi = y[random_index: random_index +1]\n",
    "              gradient = 2*xi.T @ (xi @ theta - yi)\n",
    "              theta -= learning_rate *gradient\n",
    "    return theta\n",
    "theta = np.random.randn(2,1)\n",
    "learning_rate =0.01\n",
    "n_epocs = 50\n",
    "theta_opt = stochastic_gradient(x_b,y, theta, learning_rate, n_epocs)\n",
    "print(\"Theta optimization:\",theta_opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
